{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d399b94",
   "metadata": {
    "origin_pos": 1
   },
   "source": [
    "# Lazy Initialization\n",
    ":label:`sec_lazy_init`\n",
    "\n",
    "So far, it might seem that we got away\n",
    "with being sloppy in setting up our networks.\n",
    "Specifically, we did the following unintuitive things,\n",
    "which might not seem like they should work:\n",
    "\n",
    "* We defined the network architectures\n",
    "  without specifying the input dimensionality.\n",
    "* We added layers without specifying\n",
    "  the output dimension of the previous layer.\n",
    "* We even \"initialized\" these parameters\n",
    "  before providing enough information to determine\n",
    "  how many parameters our models should contain.\n",
    "\n",
    "You might be surprised that our code runs at all.\n",
    "After all, there is no way the deep learning framework\n",
    "could tell what the input dimensionality of a network would be.\n",
    "The trick here is that the framework *defers initialization*,\n",
    "waiting until the first time we pass data through the model,\n",
    "to infer the sizes of each layer on the fly.\n",
    "\n",
    "\n",
    "Later on, when working with convolutional neural networks,\n",
    "this technique will become even more convenient\n",
    "since the input dimensionality\n",
    "(i.e., the resolution of an image)\n",
    "will affect the dimensionality\n",
    "of each subsequent layer.\n",
    "Hence, the ability to set parameters\n",
    "without the need to know,\n",
    "at the time of writing the code,\n",
    "what the dimensionality is\n",
    "can greatly simplify the task of specifying\n",
    "and subsequently modifying our models.\n",
    "Next, we go deeper into the mechanics of initialization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c21a5a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-10T06:34:41.560239Z",
     "iopub.status.busy": "2023-02-10T06:34:41.559973Z",
     "iopub.status.idle": "2023-02-10T06:34:46.654825Z",
     "shell.execute_reply": "2023-02-10T06:34:46.653249Z"
    },
    "origin_pos": 5,
    "tab": [
     "jax"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "from flax import linen as nn\n",
    "from jax import numpy as jnp\n",
    "from d2l import jax as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13673c7f",
   "metadata": {
    "origin_pos": 6
   },
   "source": [
    "To begin, let's instantiate an MLP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e66912b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-10T06:34:46.658830Z",
     "iopub.status.busy": "2023-02-10T06:34:46.658190Z",
     "iopub.status.idle": "2023-02-10T06:34:46.663199Z",
     "shell.execute_reply": "2023-02-10T06:34:46.662020Z"
    },
    "origin_pos": 10,
    "tab": [
     "jax"
    ]
   },
   "outputs": [],
   "source": [
    "net = nn.Sequential([nn.Dense(256), nn.relu, nn.Dense(10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7c3a88",
   "metadata": {
    "origin_pos": 11
   },
   "source": [
    "At this point, the network cannot possibly know\n",
    "the dimensions of the input layer's weights\n",
    "because the input dimension remains unknown.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc70a95",
   "metadata": {
    "origin_pos": 13,
    "tab": [
     "jax"
    ]
   },
   "source": [
    "As mentioned in :numref:`subsec_param-access`, parameters and the network definition are decoupled\n",
    "in Jax and Flax, and the user handles both manually. Flax models are stateless\n",
    "hence there is no `parameters` attribute.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7d714a",
   "metadata": {
    "origin_pos": 21
   },
   "source": [
    "Next let's pass data through the network\n",
    "to make the framework finally initialize parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b720fe1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-10T06:34:46.666427Z",
     "iopub.status.busy": "2023-02-10T06:34:46.666112Z",
     "iopub.status.idle": "2023-02-10T06:34:47.540759Z",
     "shell.execute_reply": "2023-02-10T06:34:47.539815Z"
    },
    "origin_pos": 25,
    "tab": [
     "jax"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(({'params': {'layers_0': {'bias': (256,), 'kernel': (20, 256)},\n",
       "    'layers_2': {'bias': (10,), 'kernel': (256, 10)}}},),\n",
       " ())"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = net.init(d2l.get_key(), jnp.zeros((2, 20)))\n",
    "jax.tree_util.tree_map(lambda x: x.shape, params).tree_flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d0bf68",
   "metadata": {
    "origin_pos": 26
   },
   "source": [
    "As soon as we know the input dimensionality,\n",
    "20,\n",
    "the framework can identify the shape of the first layer's weight matrix by plugging in the value of 20.\n",
    "Having recognized the first layer's shape, the framework proceeds\n",
    "to the second layer,\n",
    "and so on through the computational graph\n",
    "until all shapes are known.\n",
    "Note that in this case,\n",
    "only the first layer requires lazy initialization,\n",
    "but the framework initializes sequentially.\n",
    "Once all parameter shapes are known,\n",
    "the framework can finally initialize the parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d724e8",
   "metadata": {
    "origin_pos": 28,
    "tab": [
     "jax"
    ]
   },
   "source": [
    "Parameter initialization in Flax is always done manually and handled by the\n",
    "user. The following method takes a dummy input and a key dictionary as argument.\n",
    "This key dictionary has the rngs for initializing the model parameters\n",
    "and dropout rng for generating the dropout mask for the models with\n",
    "dropout layers. More about dropout will be covered later in :numref:`sec_dropout`.\n",
    "Ultimately the method initializes the model returning the parameters.\n",
    "We have been using it under the hood in the previous sections as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e406169",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-10T06:34:47.544088Z",
     "iopub.status.busy": "2023-02-10T06:34:47.543744Z",
     "iopub.status.idle": "2023-02-10T06:34:47.548560Z",
     "shell.execute_reply": "2023-02-10T06:34:47.547887Z"
    },
    "origin_pos": 30,
    "tab": [
     "jax"
    ]
   },
   "outputs": [],
   "source": [
    "@d2l.add_to_class(d2l.Module)  #@save\n",
    "def apply_init(self, dummy_input, key):\n",
    "    params = self.init(key, *dummy_input)  # dummy_input tuple unpacked\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a76ac9",
   "metadata": {
    "origin_pos": 31
   },
   "source": [
    "## Summary\n",
    "\n",
    "Lazy initialization can be convenient, allowing the framework to infer parameter shapes automatically, making it easy to modify architectures and eliminating one common source of errors.\n",
    "We can pass data through the model to make the framework finally initialize parameters.\n",
    "\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. What happens if you specify the input dimensions to the first layer but not to subsequent layers? Do you get immediate initialization?\n",
    "1. What happens if you specify mismatching dimensions?\n",
    "1. What would you need to do if you have input of varying dimensionality? Hint: look at the parameter tying.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}